version: '3.8'

services:
  # Ollama - Serves local language models
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: always

  # Open WebUI - Chat interface to interact with Ollama
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      # Mapped to port 8080 to avoid conflict with the LiteLLM UI
      - "8080:8080"
    environment:
      # Points to the ollama service within the Docker network
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - open_webui_data:/app/backend/data
    restart: always
    depends_on:
      - ollama

  # LiteLLM Proxy - Manages API keys and routes requests
  litellm-proxy:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm-proxy
    ports:
      - "4000:4000"
    environment:
      # Loads keys from the .env file
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      - LITELLM_SALT_KEY=${LITELLM_SALT_KEY}
    command: ["--port", "4000", "--host", "0.0.0.0", "--ui_port", "3000"]
    restart: always
    depends_on:
      - ollama

  # LiteLLM UI - Dashboard to view logs and usage
  litellm-ui:
    image: ghcr.io/berriai/litellm-ui:main
    container_name: litellm-ui
    ports:
      - "3000:3000"
    environment:
      # Points to the LiteLLM proxy service
      - LITELLM_PROXY_URL=http://litellm-proxy:4000
    restart: always
    depends_on:
      - litellm-proxy

# Define volumes to persist data
volumes:
  ollama_data:
  open_webui_data: